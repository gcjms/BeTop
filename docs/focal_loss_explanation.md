# Focal Loss 详解

## 1. 核心思想

普通的交叉熵 Loss 对所有样本一视同仁。
Focal Loss 的改进是：**给简单样本打折，让模型专注于困难样本**。

---

## 2. 计算步骤（伪代码）

```python
# 输入
logits = 模型输出的原始分数
target = 真值标签 (0 或 1)

# Step 1: 把分数变成概率
p = sigmoid(logits)   # 范围 [0, 1]

# Step 2: 计算"真值对应的概率"
if target == 1:
    p_t = p           # 我预测"是"的概率
else:
    p_t = 1 - p       # 我预测"否"的概率

# Step 3: 计算 Focal 权重（这是关键！）
gamma = 2             # 常用值
focal_weight = (1 - p_t) ** gamma

# Step 4: 计算最终 Loss
loss = -focal_weight * log(p_t)
```

---

## 3. 两个例子对比

### 例子 A：简单样本（预测正确且自信）

- 真值：1（确实有交互）
- 模型预测概率：0.9（很自信地说"有"）
- p_t = 0.9

计算：
- 普通 Loss = -log(0.9) = 0.105
- Focal 权重 = (1 - 0.9)^2 = 0.01
- **Focal Loss = 0.01 × 0.105 = 0.001** ← 几乎为零！

### 例子 B：困难样本（预测错误且自信）

- 真值：1（确实有交互）
- 模型预测概率：0.1（很自信地说"没有"，但错了）
- p_t = 0.1

计算：
- 普通 Loss = -log(0.1) = 2.3
- Focal 权重 = (1 - 0.1)^2 = 0.81
- **Focal Loss = 0.81 × 2.3 = 1.863** ← 基本保留！

---

## 4. 总结

| 样本类型 | 普通 Loss | Focal Loss | 效果 |
|----------|-----------|------------|------|
| 简单（预测对了） | 0.105 | 0.001 | 降低 100 倍 |
| 困难（预测错了） | 2.3 | 1.863 | 基本不变 |

**结论**：Focal Loss 通过乘一个"惩罚系数"，让模型不再浪费精力在已经学会的简单样本上，而是把注意力集中在那些还没学会的困难样本。
